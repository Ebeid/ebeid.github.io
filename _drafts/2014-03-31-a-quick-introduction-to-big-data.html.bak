---
layout: post
title: A Quick Introduction to Big Data
date: '2014-03-31T07:13:00.001-05:00'
author: Ebeid Soliman ElSayed
tags: 
modified_time: '2014-04-05T05:49:39.524-05:00'
blogger_id: tag:blogger.com,1999:blog-59384554657271185.post-35819300207364183
---

<p>In this blog post I will try to give general overview, yet deep, introduction of <a href="http://en.wikipedia.org/wiki/Big_data" target="_blank">Big Data</a> for those who seek a broad understanding or just awareness of Big Data. Generally, we will be talking about the definitions of Big Data, the component technologies that makeup the Big Data landscape, and its market (vendors).</p> <p><strong>What makes Big Data important ?</strong> Before we start talking about Big Data, we need to know what makes it a hot topic lately (even in non-technical media channels). It’s interesting fact to tell: Big Data is not a new thing. Enterprises have been always collecting enormous amounts of data. The only new thing is that enterprises now keeps this data and make analysis on it. So, what has given the rise to that ? this will take us to Big Data enablers. </p> <p><strong>What enables Big Data ?</strong> One of the most important enablers is the ability to store massive amount of data on cheap commodity hardware. So instead of using the classic big expensive server boxes (this approach was led by IBM), you can build a cluster of cheap commodity machines (this approach is led by Google and Amazon). A correlating trend to this approach is the increase of hard disk capacity and processing power in commodity machines. This made keeping and analyzing data cheaper day after day. The only missing enabler was the software; and that was solved by the release of papers from Google and Amazon (and other companies) describing how they implementing their software tools that deal with Bid Data on commodity hardware. These papers have been translated into huge number of open source projects and a lot of companies around the Big Data ecosystem (the most famous one is <a href="http://www.cloudera.com" target="_blank">Cloudera</a>).</p> <p><strong>What does Big Data enable ?</strong> Given the huge amounts of data that we can keep and analyze using Big Data, classic data analysis scenarios can be enhanced and new areas of analysis have emerged (clickstream analysis and buying patterns, sentiment analysis, fraud detection, forensic analysis, healthcare research,….)</p> <p><strong>Definitions of Big Data ?</strong> You may have seen a lot of definitions of Big Data but the most widely used definition have been introduce in 2001 by analyst Doug Laney, from Gartner, and updated in 2012 to be as follows: <a title="Laney, Douglas. &quot;The Importance of 'Big Data': A Definition&quot;. Gartner" href="http://www.gartner.com/resId=2057415" target="_blank">"Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization."</a> Sometimes people refer to this definition as the three Vs. There are some characteristics that show in most Big Data projects but it doesn’t define Big Data. Some of these characteristics are: the size of data (petabyte and exabyte), using Hadoop, using distributed parallel processing. These are common characteristics among Big Data projects, but they are not a defining characteristics of Big Data project (in other words, you can have a Big Data project that doesn’t have any of these characteristics).</p> <p><strong>What is MapReduce ?</strong> You may hear the term MapReduce a lot when people talk/write about Big Data. So, what is MapReduce ? MapReduce is a programming model and an associated implementation for processing and generating large data sets. The first publication about MapReduce came from Google research in their <a title="MapReduce: Simplified Data Processing on Large Clusters" href="http://research.google.com/archive/mapreduce.html" target="_blank">paper</a> at OSDI&nbsp; ‘04. Open source projects quickly implemented algorithmic framework, and the most famous project is Hadoop. Basically, MapReduce consists of two steps: </p> <ul> <li>Map step: split the data into key/value pairs and pre-process each pair to produce a set of intermediate key/value pairs.  <li>Reduce step: aggregates the result of the Map step by merging all intermediate values associated with the same intermediate key.</li></ul> <p><strong>MapReduce Example</strong>: The most famous and classic example of MapReduce is word count. If you have a set of text files and you want to count how often words occur. Each mapper takes a line as input and breaks it into words. It then emits a key/value pair of the word and 1. Each reducer sums the counts for each word and emits a single key/value with the word and sum (which represents the number of occurrences of this word).</p> <p><strong>Big Data vs. Data Science</strong> : although both terms are now buzz words (and sometimes used interchangeably), they are different. You could find on the internet a huge amount of resources that talk about what are differences and what are the commonalities (or common grounds) between Big Data and Data Science. I would like to save your time and give you a general statement for that: Data Science utilizes what Big Data offers and apply scientific skills (statistics, machine learning, information visualization,..) and empirical expertise (domain knowledge, critical thinking,….) to the data to come up with meaningful output. Big Data alone can’t produce the same meaningful output that Data Science offers (in most cases). Data Science can’t work probably without the foundational input from Big Data (in most cases). Maybe this the reason why both terms are used a lot together without clear boundary between them.</p> <p><strong>Technologies</strong> </p> <ul> <li><a href="http://hadoop.apache.org/" target="_blank"><strong>Hadoop</strong></a> is an Apache project for reliable, scalable, distributed computing. It combines a MapReduce engine with a distributed file system (HDFS or <u>H</u>adoop <u>D</u>istributed <u>F</u>ile <u>S</u>ystem). These are the open source implementation of MapReduce and GFS (<a href="http://static.googleusercontent.com/media/research.google.com/en/us/archive/gfs-sosp2003.pdf" target="_blank">Google File System</a>). HDFS allows disks local to individual nodes (machines) in a Hadoop cluster to operate as a single pool of storage. HDFS achieves this by replicating files across various nodes so that loss of one node will not cause loss of data (by default there is 2 additional copies of each file). Hadoop typically used in combination with other tools (which forms the Hadoop stack). From bottom up this stack can be:</li> <ul> <li><strong>Hadoop</strong> MapReduce engine &amp; HDFS</li> <li><strong>Database</strong> could be any database, most commonly it is <a href="https://hbase.apache.org/" target="_blank">Apache HBase</a> or <a href="http://cassandra.apache.org/" target="_blank">Apache Cassandra</a> (both considered as NoSQL).</li> <li><strong>Query tools</strong>: <a href="http://hive.apache.org/" target="_blank">Hive</a> (uses HiveQL as query language) &amp; <a href="http://pig.apache.org/" target="_blank">Pig</a> (uses Pig Latin as a query language).</li> <ul> <li><strong>Hive</strong> provides a SQL-like abstract level over MapReduce. Has its own HDFS table file format and it’s schema-bound that is why it can act as a bridge to BI products that expect tabular data.</li></ul> <li><strong>RDBMS Import/Export tools</strong>: most known tool is <a href="http://sqoop.apache.org/" target="_blank">Sqoop</a> (SQL-to-Hadoop database import and export)</li> <li><strong>Analysis tools</strong>: most known and promising one is <a href="https://mahout.apache.org/" target="_blank">Apache Mahout</a> for doing machine learning analysis</li> <ul> <li><a href="http://spark.apache.org/" target="_blank">Apache Spark</a> is a new promising project that is not tied to the MapReduce framework and promises higher performance.</li></ul> <li><strong>Utilities</strong> there are a lot of utilities that serve a specific supporting function like <a href="http://flume.apache.org/" target="_blank">Apache Flume</a> for moving and aggregating large data.</li></ul></ul> <p><strong>Massive Parallel Processing Data Warehouse Appliances</strong> It’s similar to MapReduce in splitting jobs into sub-jobs but it doesn’t follow the same internal mechanism. It also packaged as physical appliances (more of&nbsp; the old style server boxes).</p> <p><strong>NoSQL</strong> databases is not part of Big Data nor Data Science. It is a trend of products in the data base community that try to satisfy needs that relational databases can’t satisfy. Generally these products excel at storing unstructured and semi-structured data. They sacrifice the database consistency for the availability and ease of partitioning and other features. NoSQL products fall in four categories: key-value stores, document stores, graph stores, column-based stores.</p>  